{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "S99DljgIEq9V",
        "ZiAzvt0ZX2sX",
        "TxLzFhGpagYk",
        "uX0Ts2xQflMZ",
        "BZeCRQsVf4Ch",
        "u-xQCAYageRu",
        "IUFyB5_WhJG7",
        "ad-QMaJ8hqwf",
        "2rdETqt8hwc6",
        "yW2gExs4iaAY",
        "77uxXi5jj5lQ",
        "xQEfNw2Kj-lU",
        "neh3-l7MkZrI",
        "9bYGOzNFlDyE",
        "GR13XZ1HlIiY",
        "Zk1_o1yUlTqZ",
        "QOBfY5yyl6hG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a TensorFlow Lite based computer vision emoji input device with OpenMV\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/notebook.ipynb)\n",
        "\n",
        "\n",
        "```\n",
        "# SPDX-FileCopyrightText: Copyright 2022 Arm Limited and/or its affiliates <open-source-office@arm.com>\n",
        "# SPDX-License-Identifier: MIT\n",
        "```\n",
        "\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/00.demo.gif?raw=1\" alt=\"demo\" style=\"width: 500px\"/>\n"
      ],
      "metadata": {
        "id": "zWoDOLvrIOMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Emojis allow us to express emotions in the digital world, they are relatively easy to input on smartphone and tablet devices equipped with touch screen based virtual keyboards, but they are not as easy to input on traditional computing devices that have physical keyboards. To input emojis on these devices, users typically use a keyboard shortcut or mouse to bring up an on-screen emoji selector, and then use a mouse to select the desired emoji from a series of categories.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/01.on-screen-emoji-input-widget-on-macos.png?raw=1\" alt=\"On-screen emoji input widget on macOS\" style=\"display: block; margin-left: auto; margin-right: auto; width: 150px;\">\n",
        "<figcaption style=\"text-align: center\"><i>On-screen emoji input widget on macOS</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "This guide will explore using tinyML on an [Arm Cortex-M](https://developer.arm.com/ip-products/processors/cortex-m/) based device to create a <u>**dedicated**</u> input device. This device will take real-time input from a camera and applies a machine learning (ML) image classification model to detect if the image from the camera contains a set of known hand gestures (‚úã, üëé, üëç, üëä). When the hand gesture is detected with **high** certainty, the device will then use the [USB Human Interface Device (HID) protocol](https://en.wikipedia.org/wiki/USB_human_interface_device_class) to ‚Äútype‚Äù the emoji on the PC.\n",
        "\n",
        "The [TensorFlow Lite for Microcontrollers](https://www.tensorflow.org/lite/microcontrollers) run-time with [Arm CMSIS-NN](https://arm-software.github.io/CMSIS_5/NN/html/index.html) will be used as the on-device ML inferencing framework on the dedicated input device. On-device inferencing will allow us to <u>reduce</u> the latency of the system, as the image data will be processed at the source (instead of being transmitted to a cloud service). The user‚Äôs privacy will also be preserved, as no image data will leave the device at inference time.\n",
        "\n",
        "All technical assets for this guide can be found on [GitHub](https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv)."
      ],
      "metadata": {
        "id": "sCxQ2hUxy6dA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Microcontrollers and Keyboards\n",
        "\n",
        "Microcontroller Units (MCUs) are self-contained computing systems embedded in the devices you use every day, including your keyboard! Like all computing systems, they have inputs and outputs.\n",
        "\n",
        "The MCU inside a USB keyboard reacts to the digital events that occur when one or more of the key switches on the keyboard are pressed or released. The MCU determines which key(s) triggered the event and then translates the event into a USB HID message to send to the PC using the USB standard.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/02.block-diagram-of-usb-keyboard.png?raw=1\" alt=\"Block diagram of USB keyboard\" style=\"display: block; margin-left: auto; margin-right: auto; width: 500px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Block diagram of USB keyboard</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "Keyboard specific USB HID messages have a fixed length of 8 bytes. The first byte is composed of the status of the modifier keys (control, shift, alt) and after a padding byte, the remaining bytes indicate which keys are currently pressed.\n",
        "\n",
        "<figure >\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/03.wireshark-capture-of-usb-hid-device.png?raw=1\" alt=\"Wireshark capture of USB HID device sending CTRL+SHIFT+U key sequence\" style=\"display: block; margin-left: auto; margin-right: auto; width: 500px\">\n",
        "<figcaption style=\"text-align: center\"><i>Wireshark capture of USB HID device sending CTRL+SHIFT+U key sequence</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "The emoji ‚Äòkeyboard‚Äô will use an image sensor for input (instead of key switches) and then process the image data locally on a more powerful [Arm Cortex-M7](https://developer.arm.com/Processors/Cortex-M7) based microcontroller. All operations, including ML inferencing, are performed on a [STM32H7 MCU](https://www.st.com/en/microcontrollers-microprocessors/stm32h743vi.html), which contains an Arm Cortex-M7 CPU along with a digital interface for the image sensor and USB communications.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/04.block-diagram-of-computer-vision-based-emoji-keyboard.png?raw=1\"  alt=\"Block diagram of computer vision based emoji keyboard\" style=\"display: block; margin-left: auto; margin-right: auto; width: 500px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Block diagram of computer vision based emoji ‚Äúkeyboard‚Äù</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "Even though the STM32 H7 is a constrained computing platform that runs at 480 MHz with 1 MB of on-board RAM - we can still process a grayscale 96x96 pixel image input from the camera at just under 20 frames per second (fps)!\n"
      ],
      "metadata": {
        "id": "QVZIr5J9OOcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The OpenMV development platform\n",
        "\n",
        "[OpenMV](https://openmv.io) is an open source (Micro) Python powered Machine Vision platform. The [OpenMV product line-up](https://openmv.io/collections/cams) consists of several Arm Cortex-M based development boards. Each board is equipped with an on-board camera and MCU.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/05.screenshot-of-openmv-camera-products-page.png?raw=1\" alt=\"Screenshot of OpenMV camera products page\" style=\"display: block; margin-left: auto; margin-right: auto; width: 400px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Screenshot of <a href=\"https://openmv.io/collections/cams\">OpenMV camera products page</a></i></figcaption>\n",
        "</figure>\n",
        "\n",
        "The development boards can be used in conjunction with the [OpenMV IDE](https://openmv.io/pages/download) to develop machine vision applications. The [OpenMV run-time](https://github.com/openmv/micropython) is based on [MicroPython](https://micropython.org/), which is an implementation of the Python 3 programming language that runs on several Arm Cortex-M based microcontrollers (MCUs).\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/06.screenshot-of-openmv-ide.png?raw=1\" alt=\"Screenshot of OpenMV IDE\" style=\"display: block; margin-left: auto; margin-right: auto; width: 500px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Screenshot of OpenMV IDE</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "For this project, the [OpenMV Cam H7](https://openmv.io/products/openmv-cam-h7) or [OpenMV Cam H7 R2](https://openmv.io/collections/products/products/openmv-cam-h7-r2) board will suit our needs. Both boards are based on the STM32H7 MCU, the updated R2 revision uses an [MT9M114](https://www.onsemi.com/products/sensors/image-sensors/mt9m114) image sensor instead of the [OV7725](https://www.ovt.com/sensor/ov7725/) image sensor that is used in the original version. Both versions will work well for this project, as mentioned in the [OpenMV ‚ÄúProduction Update‚Äù blog from June 2021](https://openmv.io/blogs/news/production-update), the MT9M114 sensor offers improved image quality over the OV7725 sensor."
      ],
      "metadata": {
        "id": "eHxx0fobOX3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What we will need\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/07.openmv-cam-h7-camera-and-microsd-card.jpg?raw=1\" alt=\"OpenMV Cam H7 Camera (left) and microSD card (right)\" style=\"display: block; margin-left: auto; margin-right: auto; width: 300px;\">\n",
        "<figcaption style=\"text-align: center\"><i>OpenMV Cam H7 Camera (left) and microSD card (right)</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "* Hardware\n",
        "  * [OpenMV Cam H7](https://openmv.io/products/openmv-cam-h7) or [OpenMV Cam H7 R2](https://openmv.io/collections/products/products/openmv-cam-h7-r2) board\n",
        "  * MicroSD card with at least 2 MB of storage space (to store ML model)\n",
        "  * USB micro cable\n",
        "* Software\n",
        "  * [OpenMV IDE](https://openmv.io/pages/download)\n",
        "* Services\n",
        "  * [Google Colab](https://colab.research.google.com)\n",
        "  * [Kaggle Account](https://www.kaggle.com)"
      ],
      "metadata": {
        "id": "6BtdD5LcOe2U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "Production grade ML models are typically trained on thousands of hours of human labeled data. It would be very time consuming to collect thousands of hours of training data ourselves for this project. However, we can leverage an existing public dataset that contains 10k+ images.\n",
        "\n",
        "[Kaggle](https://www.kaggle.com) user [Sparsh Gupta (@imsparsh)](https://www.kaggle.com/imsparsh) has previously curated and shared an excellent [Gesture Recognition dataset](https://www.kaggle.com/datasets/imsparsh/gesture-recognition) and made it publicly available on [Kaggle under a permissive CC0 1.0 Universal (CC0 1.0) Public Domain license](https://creativecommons.org/publicdomain/zero/1.0/).\n",
        "\n",
        "The dataset contains ~23k image files of people performing the following hand gestures over a 30 second period:\n",
        "\n",
        "1. Left hand swipe\n",
        "2. Right hand swipe\n",
        "3. Thumbs down\n",
        "4. Thumbs up\n",
        "\n",
        "A [Kaggle account](https://www.kaggle.com) is needed to download the dataset via the [*kaggle* CLI tool](https://github.com/Kaggle/kaggle-api). Follow the instructions in [the ‚ÄúAuthentication‚Äù section of the ‚ÄúHow to Use Kaggle‚Äù guide](https://www.kaggle.com/docs/api#getting-started-installation-&-authentication) to download your account specific `kaggle.json` file, which contains your Kaggle username and API key, and place it in the correct location for the *kaggle* CLI tool to access.\n",
        " \n",
        "The Kaggle CLI can be installed via `pip`:\n"
      ],
      "metadata": {
        "id": "AUGKe2aryEw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kaggle"
      ],
      "metadata": {
        "id": "Pbs2dOzfRPCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then run the code cell below and upload your `kaggle.json` file:"
      ],
      "metadata": {
        "id": "F4NL_TBwRYKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import google.colab as colab\n",
        "\n",
        "kaggle_json = 'kaggle.json'\n",
        "\n",
        "print(f\"Please upload your '{kaggle_json}' file:\")\n",
        "uploaded = colab.files.upload()\n",
        "\n",
        "if kaggle_json not in uploaded:\n",
        "  raise Exception(f\"{kaggle_json} file was NOT uploaded!\")\n",
        "\n",
        "dot_kaggle_path = os.path.join(\n",
        "    os.path.expanduser('~'),\n",
        "    '.kaggle'\n",
        ")\n",
        "kaggle_json_path = os.path.join(dot_kaggle_path, kaggle_json)\n",
        "\n",
        "print(f'Moving kaggle.json to {kaggle_json_path}')\n",
        "os.makedirs(dot_kaggle_path, exist_ok=True)\n",
        "shutil.move(kaggle_json, kaggle_json_path)\n",
        "os.chmod(kaggle_json_path, 0o600)"
      ],
      "metadata": {
        "id": "K2POLd7GRTFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your Kaggle authentication has be setup, the dataset can be downloaded using:"
      ],
      "metadata": {
        "id": "FXOaQ5KeRiee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "kaggle datasets download --unzip --path dataset_raw imsparsh/gesture-recognition"
      ],
      "metadata": {
        "id": "sy_zzDnhRh80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect Dataset\n",
        "\n",
        "Install the `pandas` and `matplotlib` libraries:"
      ],
      "metadata": {
        "id": "UwCMmDekFRME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pandas matplotlib"
      ],
      "metadata": {
        "id": "5XNeb76uvp0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load `train.csv` with `pandas`:"
      ],
      "metadata": {
        "id": "f4HuxBGKtQ07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "train_csv_file = os.path.join('dataset_raw', 'train.csv')\n",
        "train_df = pd.read_csv(train_csv_file, sep=';', names=['folder', 'name', 'label'])\n",
        "train_df = train_df.drop(['name'], axis=1)\n",
        "\n",
        "train_df.head(-1)"
      ],
      "metadata": {
        "id": "gupJ1Cjt1H3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define function to get image paths for a folder in the dataset:"
      ],
      "metadata": {
        "id": "vLLOBQFTtga5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def image_paths_for_train_folder(folder):\n",
        "  folder_path = os.path.join('dataset_raw', 'train', folder)\n",
        "  files = os.listdir(folder_path)\n",
        "\n",
        "  return [ os.path.join(folder_path, file) for file in files ]"
      ],
      "metadata": {
        "id": "S87cQk9RpLpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display some of the images in the dataset:"
      ],
      "metadata": {
        "id": "7W6OmZSAtmZ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "oOScwRg_4B36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "fig, ax = plt.subplots(3, 3, figsize=(16, 12))\n",
        "fig.tight_layout(pad=0)\n",
        "\n",
        "for i in range(len(ax)):\n",
        "  for j in range(len(ax[0])):\n",
        "    df_index = random.randrange(len(train_df))\n",
        "\n",
        "    folder = train_df['folder'][df_index]\n",
        "\n",
        "    image_paths = image_paths_for_train_folder(folder)\n",
        "\n",
        "    image_paths_index = random.randrange(len(image_paths))\n",
        "    im = matplotlib.image.imread(image_paths[image_paths_index])\n",
        "\n",
        "    ax[i, j].imshow(im)\n",
        "    ax[i, j].text(0, -5, f\"{image_paths[image_paths_index].split(os.path.sep)[-1]}\", fontsize=8)\n",
        "    ax[i, j].axis('off')\n"
      ],
      "metadata": {
        "id": "HeTlC4AAfInq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adapting the dataset\n",
        "\n",
        "The image classification model we create will classify images into the following categories:\n",
        "\n",
        "* üö´ - No gesture\n",
        "* ‚úã - Hand up\n",
        "* üëé - Thumbs Down\n",
        "* üëç - Thumbs Up\n",
        "* üëä - Fist\n",
        "\n",
        "The swipe right and swipe left gestures in the Kaggle dataset do not correspond to any of these classes, any images in these classes will need to be discarded for our model.\n",
        "\n",
        "Since the images in the Kaggle dataset are taken over a 30 second period, they might contain other gestures at the start or end of the series. For example, some of the people in the dataset started with their hands in a fist position before eventually going to the labeled gesture hand up, thumbs up and thumbs down. Other times the person in the dataset starts off with no hand gesture in frame.\n"
      ],
      "metadata": {
        "id": "4TYOpBloSL8o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create function to animate images in a folder:"
      ],
      "metadata": {
        "id": "ubhk29UQt2GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import animation\n",
        "\n",
        "# enable HTML5 output for matplotlib animations (needed for Colab)\n",
        "matplotlib.rc('animation', html='html5')\n",
        "\n",
        "def animate_train_folder(folder, frame_interval=33):\n",
        "  fig, ax = plt.subplots(1, 1)\n",
        "  fig.tight_layout(pad=0)\n",
        "  ax.axis('off')\n",
        "  plt.close()\n",
        "\n",
        "  image_paths = image_paths_for_train_folder(folder)\n",
        "\n",
        "  ims = []\n",
        "\n",
        "  for image_path in image_paths:\n",
        "    im = matplotlib.image.imread(image_path)\n",
        "\n",
        "    ims.append([\n",
        "                ax.imshow(im)\n",
        "    ])\n",
        "\n",
        "  return animation.ArtistAnimation(\n",
        "      fig,\n",
        "      ims, \n",
        "      interval=frame_interval,\n",
        "      blit=True\n",
        "  )"
      ],
      "metadata": {
        "id": "r5DPy4Zbt6an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Animate some folders ..."
      ],
      "metadata": {
        "id": "ZYuCqV6jy-9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ani1 = animate_train_folder('WIN_20180925_17_31_48_Pro_Stop_new', frame_interval=1000)\n",
        "\n",
        "ani1.save(\"ani1.gif\", dpi=300, writer=matplotlib.animation.PillowWriter(fps=1))\n",
        "\n",
        "ani1"
      ],
      "metadata": {
        "id": "WA0d6ohGwXZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ani2 = animate_train_folder('WIN_20180925_17_34_05_Pro_Thumbs_Down_new', frame_interval=1000)\n",
        "\n",
        "ani2.save(\"ani2.gif\", dpi=300, writer=matplotlib.animation.PillowWriter(fps=1))\n",
        "\n",
        "ani2"
      ],
      "metadata": {
        "id": "-u8FTwbCvYa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ani3 = animate_train_folder('WIN_20180925_17_41_54_Pro_Thumbs_Up_new', frame_interval=1000)\n",
        "\n",
        "ani3.save(\"ani3.gif\", dpi=300, writer=matplotlib.animation.PillowWriter(fps=1))\n",
        "\n",
        "ani3"
      ],
      "metadata": {
        "id": "OHDeIcjgwiXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We‚Äôve gone ahead and manually re-labeled the images into the classes, it can be found in CSV format in the [data folder on GitHub](https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/tree/main/data), and contains labels for ~14k images."
      ],
      "metadata": {
        "id": "pg14RQYlzHBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The repository can be cloned:"
      ],
      "metadata": {
        "id": "DV24eZm-zYN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv.git\n",
        "ln -s ml-image-classification-example-for-openmv/* ."
      ],
      "metadata": {
        "id": "3ZL1_Lqvzbl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TensorFlow model\n",
        "\n",
        "We can now use TensorFlow to create and train the image classification model.\n",
        "\n",
        "Install `tensorflow`:"
      ],
      "metadata": {
        "id": "LMEVFRU-Sp1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install tensorflow==2.8.2"
      ],
      "metadata": {
        "id": "1UX-Ldkmv0iA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Images\n",
        "In order to load images from the dataset using the Keras [tf.keras.utils.image_dataset_from_directory(...)](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) API the images files need to be reorganized into the following directory structure:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "    train/\n",
        "        0/\n",
        "        1/\n",
        "        2/\n",
        "        3/\n",
        "        4/\n",
        "    val/\n",
        "        ‚Ä¶\n",
        "```\n",
        "\n",
        "The [CSV files](https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/tree/main/data) that contain the relabeled class information can be loaded using the [pandas](https://pandas.pydata.org) library and Python code can be run to set up this folder structure for the images."
      ],
      "metadata": {
        "id": "PD-rGsPVSyX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "for dataset in ['train', 'val']:\n",
        "  csv_file = os.path.join('data', f'{dataset}.csv')\n",
        "\n",
        "  df = pd.read_csv(csv_file)\n",
        "\n",
        "  for index, row in df.iterrows():\n",
        "    label = row['label']\n",
        "\n",
        "    target_dir = os.path.join('dataset', dataset, str(label))\n",
        "    source_path = os.path.join('dataset_raw', row['path'])\n",
        "    \n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "    \n",
        "    shutil.copy2(source_path, target_dir)"
      ],
      "metadata": {
        "id": "zWBtKaePzLLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TensorFlow library can in imported and used to set a random seed."
      ],
      "metadata": {
        "id": "_lYTHc6sCmn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "3MWuDC_-Cfek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "\n",
        "tf.keras.utils.set_random_seed(SEED)"
      ],
      "metadata": {
        "id": "CIP4Qw1p_NVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the images are in the correct folder structure the training images can loaded as a [TensorFlow Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset):"
      ],
      "metadata": {
        "id": "YDNZwuVyTvtb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VALIDATION_SPLIT = 0.20\n",
        "\n",
        "TRAIN_IMAGE_SIZE = (160, 120)\n",
        "IMAGE_SIZE = (96, 96)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  'dataset/train',\n",
        "  validation_split=VALIDATION_SPLIT,\n",
        "  subset='training',\n",
        "  seed=SEED,\n",
        "  image_size=TRAIN_IMAGE_SIZE,\n",
        "  color_mode='grayscale',\n",
        "  crop_to_aspect_ratio=True,\n",
        "  label_mode='categorical',\n",
        "  batch_size=BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "9mZ52DXRCsQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will reserve 20% of the items in the train folder for the validation dataset. The images will be resized 120x120 pixels (while preserving their aspect ratio) and converted from the RGB colorspace to grayscale.\n",
        "\n",
        "The validation and test datasets will re-size the images to 96x96 pixels instead of 120x120. The training dataset uses higher image dimensions to allow for data augmentation steps that will be discussed shortly.\n"
      ],
      "metadata": {
        "id": "UQAMPMPdT2Zg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  'dataset/train',\n",
        "  validation_split=VALIDATION_SPLIT,\n",
        "  subset='validation',\n",
        "  seed=SEED,\n",
        "  image_size=IMAGE_SIZE,\n",
        "  color_mode='grayscale',\n",
        "  crop_to_aspect_ratio=True,\n",
        "  label_mode='categorical',\n",
        "  batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  'dataset/val',\n",
        "  seed=SEED,\n",
        "  image_size=IMAGE_SIZE,\n",
        "  color_mode='grayscale',\n",
        "  crop_to_aspect_ratio=True,\n",
        "  label_mode='categorical',\n",
        "  batch_size=BATCH_SIZE\n",
        ")"
      ],
      "metadata": {
        "id": "GyK_RwAGT3Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of classes in the dataset can determined using:"
      ],
      "metadata": {
        "id": "_b887cC_D1r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_spec, output_spec = train_ds.element_spec\n",
        "\n",
        "num_classes = output_spec.shape[1]\n",
        "\n",
        "print('num_classes =', num_classes)"
      ],
      "metadata": {
        "id": "weq3PAH5D5-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Structure\n",
        "\n",
        "[MobileNetV1](https://arxiv.org/abs/1704.04861) is a well-known model architecture used for image classification tasks, including the [TensorLite for Microcontrollers Person detection example](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/person_detection). We‚Äôll train this model architecture on our dataset, with the same alpha (0.25) and image sizes (96x96x1) used in the [Visual Wake Words Dataset](https://arxiv.org/abs/1906.05721) paper.\n",
        "\n",
        "A MobileNetV1 model is composed of 28 layers, but a single call to the Keras [tf.keras.applications.mobilenet.MobileNet(...)](https://www.tensorflow.org/api_docs/python/tf/keras/applications/mobilenet/MobileNet) API can be used to easily create a MobileNetV1 model for 5 output classes and the desired alpha and input shape values:\n"
      ],
      "metadata": {
        "id": "dFnKgxmEEQCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ALPHA = 0.25\n",
        "DROPOUT = 0.10\n",
        "\n",
        "mobilenet_025_96 = tf.keras.applications.mobilenet.MobileNet(\n",
        "    input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 1),\n",
        "    alpha=ALPHA,\n",
        "    dropout=DROPOUT,\n",
        "    weights=None,\n",
        "    pooling='avg',\n",
        "    classes=num_classes,\n",
        ")\n",
        "\n",
        "mobilenet_025_96.summary()"
      ],
      "metadata": {
        "id": "OahtLVrQENMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### OpenMV Compatibility\n",
        "\n",
        "The MicroPython based firmware used on the OpenMV Cam H7 does not include support for all of the layer types in the MobileNetV1 model we just created using the Keras API. At the time of writing, the latest firmware version [v4.3.1](https://github.com/openmv/openmv/releases/tag/v4.3.1), only included support for the following layer types:\n",
        "\n",
        "* Add\n",
        "* AveragePool2D\n",
        "* Conv2D\n",
        "* DepthwiseConv2D\n",
        "* FullyConnected\n",
        "* MaxPool2D\n",
        "* Mean\n",
        "* Pad\n",
        "* Reshape\n",
        "* Shape\n",
        "* Softmax\n",
        "* Sub\n",
        "\n",
        "This was done to decrease the size of the OpenMV run-time to fit within the STM32 H7‚Äôs 2 MB of flash memory. More details can be found in the [libtf.cc file on GitHub](https://github.com/openmv/tensorflow-lib/blob/master/libtf.cc).\n",
        "\n",
        "We‚Äôll need to adapt the model as follows:\n",
        "\n",
        "* Drop any [ZeroPadding2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ZeroPadding2D) layers\n",
        "* Modify the [DepthwiseConv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/DepthwiseConv2D) layers to use *‚Äòsame‚Äô* padding\n",
        "* Replace [GlobalAveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D) layers with [AveragePooling2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D) layers\n",
        "*Replace [Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape) layers with [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) layers\n",
        "\n",
        "This can be done in only ~30 lines of Python code:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S99DljgIEq9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# only support some operations: https://github.com/openmv/tensorflow-lib/blob/master/libtf.cc\n",
        "def modify_mobilenet_for_openmv(mobilenet_model):\n",
        "  input_type_spec = mobilenet_model.layers[0].input.type_spec\n",
        "\n",
        "  input = tf.keras.Input(shape=(input_type_spec.shape[1:]))\n",
        "  output = input\n",
        "\n",
        "  for layer in mobilenet_model.layers[1:]:\n",
        "    if (isinstance(layer, tf.keras.layers.ZeroPadding2D)):\n",
        "      print(\"dropping ZeroPadding2D\", layer.name)\n",
        "    elif (isinstance(layer, tf.keras.layers.DepthwiseConv2D)) and layer.padding != 'same':\n",
        "      print(\"replacing DepthwiseConv2D\", layer.name)\n",
        "      output = tf.keras.layers.DepthwiseConv2D(\n",
        "            kernel_size=layer.kernel_size,\n",
        "            strides=layer.strides,\n",
        "            padding='same',\n",
        "            depth_multiplier=layer.depth_multiplier,\n",
        "            use_bias=layer.use_bias,\n",
        "            name=layer.name\n",
        "      )(output)\n",
        "    elif (isinstance(layer, tf.keras.layers.GlobalAveragePooling2D)):\n",
        "      print(\"replacing GlobalAveragePooling2D\", layer.name)\n",
        "      output = tf.keras.layers.AveragePooling2D((3, 3), strides=(2, 2), padding='valid')(output)\n",
        "    elif (isinstance(layer, tf.keras.layers.Reshape)):\n",
        "      output = tf.keras.layers.Flatten()(output)\n",
        "    else:\n",
        "      output = layer(output)\n",
        "\n",
        "  return tf.keras.Model(input, output)"
      ],
      "metadata": {
        "id": "zHm1UgquFZPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openmv_mobilenet_025_96 = modify_mobilenet_for_openmv(mobilenet_025_96)\n",
        "\n",
        "openmv_mobilenet_025_96.summary()"
      ],
      "metadata": {
        "id": "UAlwR4gE27St"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this adaptation is done, the model structure will also be identical to the TensorFlow Lite Person Detection example model, apart from the number of outputs the model has (5 vs 1) in the final layer."
      ],
      "metadata": {
        "id": "ZCcQ-74ZU2pl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Augmentation\n",
        "\n",
        "In order to avoid the model overfitting the training dataset, we can introduce a data augmentation step to alter input images during training.\n",
        "\n",
        "We will use the following built-in Keras layers for this:\n",
        "* [Random flipping](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomFlip)\n",
        "* [Random rotation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomRotation)\n",
        "* [Random zooming](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomZoom)\n",
        "* [Random contrast adjustments](https://www.tensorflow.org/api_docs/python/tf/keras/layers/RandomContrast)\n"
      ],
      "metadata": {
        "id": "_hbBeJ5WFeeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = tf.keras.Sequential([\n",
        "  tf.keras.layers.RandomFlip(\"horizontal\", seed=SEED),\n",
        "  tf.keras.layers.RandomRotation(0.1, seed=SEED),\n",
        "  tf.keras.layers.RandomZoom(0.1, seed=SEED),\n",
        "  tf.keras.layers.RandomContrast(0.2, seed=SEED),\n",
        "])"
      ],
      "metadata": {
        "id": "YyJzYnioFhHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combine layers\n",
        "\n",
        "The adapted MobileNetV1 model and data augmentation ([Sequential](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)) layer can then be combined as follows:"
      ],
      "metadata": {
        "id": "DwJo1GG4FtoY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "  data_augmentation,\n",
        "  tf.keras.layers.Resizing(IMAGE_SIZE[0], IMAGE_SIZE[1], crop_to_aspect_ratio=True),\n",
        "  openmv_mobilenet_025_96\n",
        "])"
      ],
      "metadata": {
        "id": "IY1MyFbmFswr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A [Resizing](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Resizing) layer was added to resize the output of the data augmentation layer to match the 96x96 input size required by the MobileNetV1 model."
      ],
      "metadata": {
        "id": "w5ePoLwTV0yd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train\n",
        "\n",
        "We are now ready to train the model. To start, we can define two callbacks used during training:\n",
        "\n",
        "1. A [LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler) callback to exponentially decrease the learning rate after each epoch.\n",
        "\n",
        "2. A [ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint) callback to save the weights with the best validation loss."
      ],
      "metadata": {
        "id": "KuMsjIS0GxCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 0.01\n",
        "EPOCHS = 20\n",
        "\n",
        "callbacks = [\n",
        "  tf.keras.callbacks.LearningRateScheduler(\n",
        "      schedule=lambda epoch, lr: lr * tf.math.exp(-0.1)\n",
        "  ),\n",
        "  tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='/tmp/checkpoint',\n",
        "    monitor='val_loss',\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        "    mode='min',\n",
        "    save_weights_only=True,\n",
        "  )\n",
        "]"
      ],
      "metadata": {
        "id": "XomHDcUnGz7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " \n",
        "The model can then be compiled with an [Adam](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam) optimizer and [CategoricalCrossEntropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) loss function. The starting learning rate will be set to 0.1."
      ],
      "metadata": {
        "id": "BLOAlqXmWrde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
        "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "csFc6Tn-WtnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally model.fit(...) can be called to train on the dataset for 20 epochs. *Alternatively, to save time you skip the next 4 code cells and download a pre-trained model.*"
      ],
      "metadata": {
        "id": "Hs0U3zs5W3bL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    train_ds.cache().prefetch(BATCH_SIZE),\n",
        "    validation_data=val_ds.cache().prefetch(BATCH_SIZE),\n",
        "    epochs=EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    callbacks=callbacks\n",
        ")"
      ],
      "metadata": {
        "id": "058Qk2--W6e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model has completed training, the best weights found during training for the validation loss can be restored as follows:"
      ],
      "metadata": {
        "id": "UbodSrP_WJPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('/tmp/checkpoint')"
      ],
      "metadata": {
        "id": "iRMDzecVN8Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and then the model can be saved:"
      ],
      "metadata": {
        "id": "iGj-1FlgHcLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('model')"
      ],
      "metadata": {
        "id": "GdKfqcAkHDV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and zipped up:"
      ],
      "metadata": {
        "id": "dPXKBLUdWQkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "zip -r model.zip model/"
      ],
      "metadata": {
        "id": "CJ1_wRK5jPRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If needed, the pre-trained model from GitHub can be [downloaded](https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/archive/refs/heads/pretrained.zip) and restored by uncommenting and running:"
      ],
      "metadata": {
        "id": "1FmjRoXZWZD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.get_file(\n",
        "#     fname='model.zip',\n",
        "#     origin='https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/archive/refs/heads/pretrained.zip',\n",
        "#     extract=True,\n",
        "#     cache_subdir='/content'\n",
        "# )\n",
        "\n",
        "# model = tf.keras.models.load_model('ml-image-classification-example-for-openmv-pretrained/model')\n",
        "\n",
        "# openmv_mobilenet_025_96 = model.layers[-1]"
      ],
      "metadata": {
        "id": "p2wJsP7sYdi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate\n",
        "\n",
        "The test dataset can then be used to evaluate performance of the trained model:"
      ],
      "metadata": {
        "id": "kmBYYMptIHWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "kzIFvdGMIW_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An accuracy metric of ~0.65 was obtained which is not great but is good enough to continue."
      ],
      "metadata": {
        "id": "8WrFMKzmXMCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Uncertainty\n",
        "\n",
        "[Robert Monarch‚Äôs ‚ÄúHuman-in-the-Loop Machine Learning‚Äù book](https://www.manning.com/books/human-in-the-loop-machine-learning) is an excellent resource for identifying which samples to prioritize for human labeling in human in the loop machine learning systems. Chapters 3 and 4 introduce concepts of ‚Äúlow activation‚Äù and ‚Äúuncertainty sampling‚Äù to identify when an ML model is uncertain about its input data. We will try to leverage these concepts at inference time on our model to understand when it is certain about its outputs."
      ],
      "metadata": {
        "id": "2mcyhgF8IZxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Softmax function and low activation inputs\n",
        "\n",
        "The model we have trained uses a Softmax function in the final layer and has a formula of:\n",
        "\n",
        "\\begin{align}\n",
        "    \\sigma({z_i}) = \\dfrac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
        "\\end{align}\n",
        "\n",
        "Since the function is dividing by an exponential, it will lose the scale of the logit inputs.\n",
        "\n",
        "For example:\n",
        "\n",
        "1. `softmax([-2, 1, -1, 0])`\n",
        "2. `softmax([1, 4, 2, 3])`\n",
        "3. `softmax([11, 14, 12, 13])`\n",
        "4. `softmax([101, 104, 102, 103])`\n",
        "\n",
        "all have an output value of `[0.0320586 , 0.64391426, 0.08714432, 0.23688282]` even though the scale of the input values have different magnitudes.\n",
        "\n",
        "When looking at the inputs to the softmax function from the perspective of the output of a hidden layer in a neural network, the first two example inputs would be considered to have ‚Äúlower activation‚Äù compared to the last two example inputs. Having access to the inputs of the softmax activation layer during inference can give us insights into how activated the model‚Äôs hidden layer is and help to decide how certain the model‚Äôs output is.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/10-hidden-layer-outputs-feeding-into-softmax-layer.png?raw=1\" alt=\"OpenMV Cam H7 Camera (left) and microSD card (right)\" style=\"display: block; margin-left: auto; margin-right: auto; width: 600px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Hidden Layer Outputs feeding into Softmax layer</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "For example, we can set the minimum threshold for the maximum entry in the hidden layer output to be 5 and mark all model outputs below this criteria as uncertain.\n"
      ],
      "metadata": {
        "id": "ZiAzvt0ZX2sX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Uncertainty sampling \n",
        "\n",
        "Uncertainty sampling techniques allow you to detect when the model‚Äôs output is near a decision boundary. One technique for doing this is called ‚ÄúMargin of confidence sampling‚Äù, this technique uses the difference between the top two most confident predictions. This difference can give us some perspective on the certainty of a model's outputs."
      ],
      "metadata": {
        "id": "TxLzFhGpagYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Getting a certainty insights\n",
        "\n",
        "Next we will see how we get the hidden layer outputs in Keras. This can be done by creating a new model with layer 0‚Äôs input tensor as the input and the second last layers output tensor as the output:"
      ],
      "metadata": {
        "id": "UemamHCLcL7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_layer_openmv_mobilenet_025_96 = tf.keras.Model(\n",
        "    inputs=[\n",
        "            openmv_mobilenet_025_96.layers[0].input\n",
        "    ],\n",
        "    outputs=[\n",
        "             openmv_mobilenet_025_96.layers[-2].output        \n",
        "    ]\n",
        ")\n",
        "\n",
        "hidden_layer_openmv_mobilenet_025_96.summary()"
      ],
      "metadata": {
        "id": "7lziDPwEikJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extend this further and create a model with multiple outputs:\n",
        "\n",
        "1. The original output\n",
        "2. The output of the hidden layer\n",
        "3. The predicted label - based on the index with the maximum value\n",
        "4. The predicted confidence - based on the maximum softmax output value\n",
        "5. The maximum value of the hidden layer\n",
        "6. The margin of confidence"
      ],
      "metadata": {
        "id": "jG5_l6Ciard6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = openmv_mobilenet_025_96.output\n",
        "hidden_layer_output = openmv_mobilenet_025_96.layers[-2].output\n",
        "\n",
        "predicted_label_output = tf.argmax(model_output, axis=-1)\n",
        "predicted_confidence = tf.reduce_max(openmv_mobilenet_025_96.output, axis=-1)\n",
        "\n",
        "sorted_hidden_layer_output = tf.sort(hidden_layer_output, direction='DESCENDING', axis=-1)\n",
        "max_hidden_layer_output = sorted_hidden_layer_output[:, 0]\n",
        "margin_of_confidence_output = tf.math.subtract(sorted_hidden_layer_output[:, 0], sorted_hidden_layer_output[: ,1])\n",
        "\n",
        "model_with_certainty = tf.keras.Model(\n",
        "    inputs=[\n",
        "            openmv_mobilenet_025_96.layers[0].input\n",
        "    ],\n",
        "    outputs=[\n",
        "             model_output,\n",
        "             hidden_layer_output,\n",
        "             predicted_label_output,\n",
        "             predicted_confidence,\n",
        "             max_hidden_layer_output,\n",
        "             margin_of_confidence_output\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "N14KpQAhjTSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can load some images and compare outputs of the various outputs of the model."
      ],
      "metadata": {
        "id": "w3Td1JB1a0q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv_file = os.path.join('data', 'val.csv')\n",
        "test_df = pd.read_csv(test_csv_file)\n",
        "\n",
        "\n",
        "test_df['path'] = test_df['path'].apply(lambda p: os.path.join('dataset_raw', p))\n",
        "\n",
        "test_df.head()"
      ],
      "metadata": {
        "id": "VXhYcYmZk7rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_paths_ds = tf.data.Dataset.from_tensor_slices(test_df['path'])\n",
        "\n",
        "def decode_img(p):\n",
        "  img = tf.io.read_file(p)\n",
        "  img = tf.io.decode_image(img)\n",
        "  img = tf.image.rgb_to_grayscale(img)\n",
        "  img = tf.image.resize_with_crop_or_pad(img, IMAGE_SIZE[0], IMAGE_SIZE[1])\n",
        "\n",
        "  return img\n",
        "\n",
        "test_images_ds = test_paths_ds.map(decode_img)\n",
        "\n",
        "p = model_with_certainty.predict(test_images_ds.batch(len(test_df)))\n"
      ],
      "metadata": {
        "id": "1kzbpBFOIGjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['predicted_label'] = p[2]\n",
        "test_df['predicted_confidence'] = p[3]\n",
        "test_df['hidden_layer_max'] = p[4]\n",
        "test_df['margin_of_confidence'] = p[5]"
      ],
      "metadata": {
        "id": "WENE7m1i9ze0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "import numpy as np\n",
        "\n",
        "def display_test_df_row(display_index):\n",
        "  display(IPython.display.Image(test_df['path'][display_index]))\n",
        "\n",
        "  display(IPython.display.Markdown(f'''\n",
        "  | | |\n",
        "  | ---------------------------------- | ------------------------------------------------ |\n",
        "  | **Actual label**                   | {test_df['label'][display_index]}                |\n",
        "  | **Predicted label**                | {test_df['predicted_label'][display_index]}      |\n",
        "  | **Softmax output**                 | {np.around(p[0][display_index], 3)}                           |\n",
        "  | **Hidden output**                  | {np.around(p[1][display_index], 6)}                           |\n",
        "  | **Predicted confidence**           | {test_df['predicted_confidence'][display_index]} |\n",
        "  | **Maximum output of hidden layer** | {test_df['hidden_layer_max'][display_index]}     |\n",
        "  | **Margin of confidence**           | {test_df['margin_of_confidence'][display_index]} |\n",
        "  '''))"
      ],
      "metadata": {
        "id": "SGcX86XtHUhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lowest_margin_of_confidence_index = test_df.sort_values('margin_of_confidence', ascending=True).index[0]\n",
        "\n",
        "print('Lowest margin of confidence')\n",
        "display_test_df_row(lowest_margin_of_confidence_index)"
      ],
      "metadata": {
        "id": "pOmd5aUyhrfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This image has the **lowest** margin of confidence in the test dataset - you can see it is near a decision boundary between classes."
      ],
      "metadata": {
        "id": "ytKTT363bC0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "highest_margin_of_confidence_index = test_df.sort_values('margin_of_confidence', ascending=False).index[0]\n",
        "\n",
        "print('Highest margin of confidence')\n",
        "display_test_df_row(highest_margin_of_confidence_index)"
      ],
      "metadata": {
        "id": "vTLqnMJ3zxhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This image has the **highest** margin of confidence in the test dataset."
      ],
      "metadata": {
        "id": "XKQEKilmbFtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lowest_activation_index = test_df.sort_values('hidden_layer_max', ascending=True).index[0]\n",
        "\n",
        "print('Lowest activation')\n",
        "display_test_df_row(lowest_activation_index)"
      ],
      "metadata": {
        "id": "adzXr6XSz4nU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This image has the **lowest** maximum hidden layer output in the test dataset and is considered to have ‚Äúlow activation‚Äù.\n",
        "\n",
        "These example images provided a brief highlight of the certainty insights we covered earlier. Later in the guide you will have an opportunity to see how thresholds for each will impact when the inference application detects a hand gesture and ‚Äútypes‚Äù an emoji.\n"
      ],
      "metadata": {
        "id": "54sxVitcbKx1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Converting model to TensorFlow Lite format\n",
        "\n",
        "In order for the (hidden layer output of the) model to be deployed on the OpenMV Cam H7 board it first needs to be converted into [TensorFlow Lite](https://www.tensorflow.org/lite) format. This can be done using the [tf.lite.TFLiteConverter.from_keras_model(...)](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter) API. The [default optimizations](https://www.tensorflow.org/api_docs/python/tf/lite/Optimize) for conversion will be selected to enable quantized 8-bit weights. The validation dataset will be used as a representative data set for the quantization process. The model's input and output types will be set to [tf.int8](https://www.tensorflow.org/api_docs/python/tf/dtypes) data types.\n"
      ],
      "metadata": {
        "id": "avMkWePvEdt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "presoftmax_openmv_mobilenet_025_96 = tf.keras.Model(\n",
        "    inputs=[\n",
        "            openmv_mobilenet_025_96.layers[0].input\n",
        "    ],\n",
        "    outputs=[\n",
        "             openmv_mobilenet_025_96.layers[-2].output        \n",
        "    ]\n",
        ")\n",
        "\n",
        "def representative_dataset():\n",
        "  for image, label in val_ds.unbatch():\n",
        "    yield [ np.array([image]) ]\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(presoftmax_openmv_mobilenet_025_96)\n",
        "converter.optimizations = [ tf.lite.Optimize.DEFAULT ]\n",
        "converter.representative_dataset = representative_dataset\n",
        "\n",
        "# https://github.com/openmv/tensorflow-lib/blob/2abbaee8458379c83444fc391cde5e748becfd55/libtf.cc\n",
        "converter.target_spec.supported_ops = [ tf.lite.OpsSet.TFLITE_BUILTINS_INT8 ]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_quant_model = converter.convert()"
      ],
      "metadata": {
        "id": "PJxtoGDbI6VV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is converted it can then be saved to a model.tflite file to be transferred to the camera."
      ],
      "metadata": {
        "id": "jLTEz1HCbn42"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model.tflite', 'wb') as output:\n",
        "  print(len(tflite_quant_model))\n",
        "  output.write(tflite_quant_model);"
      ],
      "metadata": {
        "id": "LL5DRUuebo4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The converted model has a size of ~303 kilobytes and can be inspected using the [Netron App](https://netron.app)."
      ],
      "metadata": {
        "id": "gVgKw1GIbtcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the code cell below to download the model:"
      ],
      "metadata": {
        "id": "XvuCDvtPb0Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "colab.files.download('model.tflite')"
      ],
      "metadata": {
        "id": "FnMU63Y5j17e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenMV Application\n",
        "\n",
        "This section will outline how to set up the OpenMV (Integrated Development Environment) IDE and develop applications for the OpenMV board. It will also provide an overview of key parts of the inference application. The full source code for the application can be found in the [`openmv` folder on GitHub](https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/tree/main/openmv).\n"
      ],
      "metadata": {
        "id": "M8sEPTwefdS6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up the development environment\n",
        "\n",
        "[Download and install the OpenMV IDE](https://openmv.io/pages/download) for your operating system. More information on this can be found on the [\"OpenMV Cam Tutorial - Software Setup\" page](https://docs.openmv.io/openmvcam/tutorial/software_setup.html)."
      ],
      "metadata": {
        "id": "uX0Ts2xQflMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hardware Setup\n",
        "\n",
        "Insert the microSD card into the back of the OpenMV camera and then plug in the micro USB cable into the bottom of the board. Connect the other end of the USB cable to your computer. Once plugged in the camera will appear as a new removable USB disk drive on your computer.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/14.back-of-openmv-cam-h7-board-with-microsd-card-inserted.jpg?raw=1\" alt=\"Back of OpenMV Cam H7 board with microSD card inserted\" style=\"display: block; margin-left: auto; margin-right: auto; width: 250px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Back of OpenMV Cam H7 board with microSD card inserted</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "Start the OpenMV IDE and click on the ‚ÄúConnect‚Äù button in the bottom left corner.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/15.openmv-ide-connect-button.png?raw=1\" alt=\"OpenMV IDE - Connect button\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
        "<figcaption style=\"text-align: center\"><i>OpenMV IDE - Connect button</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "You may be prompted to update the firmware running on the board if it is not the latest version available. After this, the icon will change state to connected and the ‚ÄúRun‚Äù button below will be enabled.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/16.openmv-ide-board-connected-state.png?raw=1\" alt=\"OpenMV IDE - Board connected state\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
        "<figcaption style=\"text-align: center\"><i>OpenMV IDE - Board connected state</i></figcaption>\n",
        "</figure>"
      ],
      "metadata": {
        "id": "BZeCRQsVf4Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hello World example\n",
        "To ensure the camera is functioning correctly we can upload the ‚ÄúHello World‚Äù example onto the board. In the OpenMV IDE, select `File -> Examples -> OpenMV -> Basics -> helloworld.py`\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/17.hello_world.py-example-in-the-openmv-ide.png?raw=1\" alt=\"hello_world.py example in the OpenMV IDE\" style=\"display: block; margin-left: auto; margin-right: auto; width: 500px;\">\n",
        "<figcaption style=\"text-align: center\"><i>hello_world.py example in the OpenMV IDE</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "Now click on the ‚ÄúRun‚Äù icon in the bottom left corner of the OpenMV IDE to run the example. The board will start running the example, and you will see the view from the camera in the top right corner of the OpenMV IDE. To see the output of the `print(...)` statements in the example click on the ‚ÄúSerial Terminal‚Äù button on the bottom of the IDE. The execution of the script can be stopped by clicking the ‚ÄúStop‚Äù button in the bottom left corner of the IDE.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/18.openmv-ide-stop-button.png?raw=1\" alt=\"OpenMV IDE - Stop button\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
        "<figcaption style=\"text-align: center\"><i>OpenMV IDE - Stop button</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "We can now modify the example to match what is needed for the ML model we trained and converted earlier, by changing the following lines from:\n",
        "\n",
        "```python\n",
        "sensor.set_pixformat(sensor.RGB565)\n",
        "sensor.set_framesize(sensor.QVGA)\n",
        "```\n",
        "\n",
        "to:\n",
        "\n",
        "```python\n",
        "sensor.set_pixformat(sensor.GRAYSCALE)\n",
        "sensor.set_framesize(sensor.QQVGA)\n",
        "```\n",
        "\n",
        "These changes will make the camera obtain a grayscale image of 160x120 pixels (instead of a RGB 320x240 pixels). Press the ‚ÄúStart‚Äù button to test the changes on your board.\n",
        "\n",
        "**Note**: The [OpenMV `sensor` module documentation](https://docs.openmv.io/library/omv.sensor.html) contains information on additional API‚Äôs and options you may want to explore outside of this project."
      ],
      "metadata": {
        "id": "u-xQCAYageRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using TensorFlow with OpenMV\n",
        "\n",
        "The [OpenMV `tf` module](https://docs.openmv.io/library/omv.tf.html) enables integration of quantized TensorFlow Lite models into OpenMV applications. It uses the [TensorFlow Lite for Microcontrollers C/C++ library](https://github.com/tensorflow/tflite-micro) but wraps it in its own customized Python API. When using a model that is stored on a microSD, it must be under 400KB in size, since the model will be loaded from the SD card to the OpenMV Cam H7‚Äôs RAM.\n",
        "\n",
        "To get started copy the `model.tflite` file from earlier onto the OpenMV board‚Äôs removable USB disk interface on your computer. Once transferred it will be stored on the microSD card.\n",
        "\n",
        "We can then edit the `hello_world.py` example used earlier, first add a new import line for the `tf` module to the start of the file under the existing imports:\n",
        "\n",
        "```python\n",
        "import tf\n",
        "```\n",
        "\n",
        "Now that the `tf` module has been loaded, the model can loaded as follows before the main loop of the example:\n",
        "\n",
        "```python\n",
        "model = tf.load(\"model.tflite\", load_to_fb=True)\n",
        "```\n",
        "\n",
        "The model output for an input image (from the camera) can be calculated using the following in the main loop:\n",
        "\n",
        "```python\n",
        "classification_result = model.classify(img)\n",
        "model_output = classification_result[0].output()\n",
        "  \n",
        "print(model_output)\n",
        "```\n",
        "\n",
        "Click the ‚ÄúStart‚Äù button again to run the changes we‚Äôve made to the example. You can test how the model output varies by posing with the hand gestures the model is trained for. In my testing I found the model behaved best when the camera was on the desk in front of me and slightly tilted up, and the room was well lit up.\n",
        "\n",
        "Since the model we converted used the hidden layer outputs as its outputs, a softmax function must be used to convert the hidden layer output values to softmax output. This can be done by defining a `softmax(...)` function at the start of the file after the imports:\n",
        "\n",
        "```python\n",
        "import math\n",
        "\n",
        "def softmax(input):\n",
        "   result = []\n",
        "\n",
        "   numerator = []\n",
        "   denominator = 0\n",
        "   for i, item in enumerate(input):\n",
        "       numerator.append(math.exp(item))\n",
        "       denominator += math.exp(item)\n",
        "\n",
        "   for i, item in enumerate(numerator):\n",
        "       result.append(numerator[i] / denominator)\n",
        "\n",
        "   return result\n",
        "```\n",
        "\n",
        "The main loop can then be updated to call this new function and output softmax output values:\n",
        "\n",
        "```python\n",
        "softmax_model_output = softmax(model_output)\n",
        "  \n",
        "print(model_output, softmax_model_output)\n",
        "```\n",
        "\n",
        "The predicted class of the input image can be calculated by finding the output index with the highest values. Then the index can be used to print the associated emoji using an array which holds the string values for each class.\n",
        "\n",
        "```python\n",
        "LABELS = [\"üö´\", \"‚úã\", \"üëé\", \"üëç\", \"üëä\"]\n",
        "# ...\n",
        "while True:\n",
        "    # ...\n",
        "    classification = model_output.index(max(model_output))\n",
        "  \n",
        "    print(model_output, softmax_model_output, LABELS[classification])\n"
      ],
      "metadata": {
        "id": "IUFyB5_WhJG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating the margin of confidence of the model output\n",
        "\n",
        "We would like the system to react when the model‚Äôs output has a high degree of certainty. To do this we will calculate the model outputs margin of confidence value by sorting the models hidden layer output values and then subtracting the two highest values: \n",
        "\n",
        "```python\n",
        "sorted_model_output = model_output.copy()\n",
        "sorted_model_output.sort(reverse=True)\n",
        "margin_of_confidence = sorted_model_output[0] - sorted_model_output[1]\n",
        "```\n",
        "\n",
        "The printout we had earlier can be modified to only print label emojis for model outputs with certain above a specific class specific threshold for both activation values and margin of confidence:\n",
        "\n",
        "```python\n",
        "ACTIVATION_THRESHOLDS = [0, 6, 2, 2, 2] # activation threshold\n",
        "MOC_THRESHOLDS = [0, 5, 3, 3, 3] # margin of confidence threshold\n",
        "# ...\n",
        "\n",
        "while True:\n",
        "    # ‚Ä¶\n",
        "    \n",
        "    above_activation_threshold = (\n",
        "        sorted_model_output[0] > ACTIVATION_THRESHOLDS[classification]\n",
        "    )\n",
        "    above_moc_threshold = margin_of_confidence > MOC_THRESHOLDS[classification]\n",
        "\n",
        "    if above_activation_threshold and above_moc_threshold:\n",
        "        print(\n",
        "            model_output,\n",
        "            softmax_model_output,\n",
        "            margin_of_confidence,\n",
        "            LABELS[classification]\n",
        "        )\n",
        "    else:\n",
        "        print(model_output, softmax_model_output, margin_of_confidence)\n",
        "\n",
        "```\n",
        "\n",
        "Run the script and tune the per class threshold values so that you get a reasonable amount of gesture detected.\n"
      ],
      "metadata": {
        "id": "ad-QMaJ8hqwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RGB LED integration\n",
        "\n",
        "The OpenMV has an on-board RGB LED, we can use it to show a visual indication when a gesture is detected. This will be useful when the application is running standalone without the OpenMV IDE.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/19.rgb-led-on-openvm-%20h7-cycling-colors.gif?raw=1\" alt=\"RGB LED on OpenMV H7 cycling colors\" style=\"display: block; margin-left: auto; margin-right: auto; width: 200px;\">\n",
        "<figcaption style=\"text-align: center\"><i>RGB LED on OpenMV H7 cycling colors</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "We can map the following colors to the image classification classes:\n",
        "\n",
        "* ‚ö™Ô∏è White:  üö´ - No gesture\n",
        "* üü° Yellow: ‚úã - Hand up\n",
        "* üî¥ Red:     üëé - Thumbs Down\n",
        "* üü¢ Green:  üëç - Thumbs Up\n",
        "* üîµ Blue:     üëä - Fist\n",
        "\n",
        "The LED‚Äôs can be accessed by using the [LED class](https://docs.openmv.io/library/pyb.LED.html) inside in the [`pyb` module](https://docs.openmv.io/library/pyb.html). The red LED has an id of 1, while the green and blue LEDs are id 2 and 3 respectively: \n",
        "\n",
        "```python\n",
        "import pyb\n",
        "\n",
        "red_led = pyb.LED(1)\n",
        "green_led = pyb.LED(2)\n",
        "blue_led = pyb.LED(3)\n",
        "```\n",
        "\n",
        "Each LED can be turned on and off, by calling [`led.on()`](https://docs.openmv.io/library/pyb.LED.html#pyb.LED.on) and [`led.off()`](https://docs.openmv.io/library/pyb.LED.html#pyb.LED.off). A white color can be mixed by turning on all three LEDs at the same time, while yellow can be mixed by only turning on the red and green LEDs. We can define a function that takes input string and turns the appropriate LEDs on and off.\n",
        "\n",
        "```python\n",
        "def set_rgb_led(color):\n",
        "   red_led.on() if \"r\" in color else red_led.off()\n",
        "   green_led.on() if \"g\" in color else green_led.off()\n",
        "   blue_led.on() if \"b\" in color else blue_led.off()\n",
        "```\n",
        "\n",
        "For example, passing in ‚Äòr‚Äô will turn the red LED on, ‚Äòrg‚Äô will turn on both the red and green LEDs, and `rgb` will turn on all three LEDs.\n",
        "\n",
        "We can create a new array to store the LED string values for each class:\n",
        "\n",
        "```python\n",
        "LED_LABELS = [\"rgb\", \"rg\", \"r\", \"g\", \"b\"]\n",
        "```\n",
        "\n",
        "The if statement in the main loop can then be updated to call the function, if the model is determined to be uncertain the LEDs will be set to color of class 0 - white:\n",
        "\n",
        "```python\n",
        "if above_activation_threshold and above_moc_threshold:\n",
        "    # ...\n",
        "    set_rgb_leds(LED_LABELS[classification])\n",
        "else:\n",
        "    # ...\n",
        "    set_rgb_leds(LED_LABELS[0])\n",
        "```\n"
      ],
      "metadata": {
        "id": "2rdETqt8hwc6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exponential smoothing\n",
        "\n",
        "As you probably observed during testing so far, the outputs from the model are slightly noisy. This is due to the camera sensor being slightly noisy and this noise trickling down to the model‚Äôs output.\n",
        "\n",
        "We can use a [basic exponential smoothing function](https://en.wikipedia.org/wiki/Exponential_smoothing#Basic_(simple)_exponential_smoothing_(Holt_linear)) to smooth the output of the model prior to deciding if an emoji needs to be ‚Äútyped‚Äù on the PC. The formula is as follows:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "    S_t = \\alpha \\times X_t + (1 - \\alpha) \\times S_{t - 1}\n",
        "\\end{align}\n",
        "\n",
        "The alpha value is called the ‚Äúsmoothing factor‚Äù and is a number between 0 and 1. It controls how much influence new X<sub>t</sub> values have on the output of S<sub>t</sub>.\n",
        "\n",
        "This function can be defined in Python using:\n",
        "\n",
        "```python\n",
        "def exponential_smooth(x, s_in, alpha):\n",
        "    s_out =  [0] * len(s_in)\n",
        "\n",
        "    for i in range(len(s_in)):\n",
        "        s_out[i] = alpha * x[i] + (1 - alpha) * s_in[i]\n",
        "\n",
        "    return s_out\n",
        "```\n",
        "\n",
        "If model output does not have a high degree of certainty we can override the softmax output to `[1, 0, 0, 0, 0]` to place it in the no gesture category:\n",
        "\n",
        "```python\n",
        "if above_activation_threshold and above_moc_threshold:\n",
        "    # ...\n",
        "else:\n",
        "    # ‚Ä¶\n",
        "    softmax_model_output = [0] * len(softmax_model_output)\n",
        "    softmax_model_output[0] = 1\n",
        "```\n",
        "\n",
        "Then the `softmax_model_ouput` variable can be exponentially smoothed into a new variable called `smoothed_softmax_model_output`, for this application we will use an alpha value of 0.20.\n",
        "\n",
        "```python\n",
        "ALPHA = 0.20\n",
        "\n",
        "# ...\n",
        "\n",
        "smoothed_softmax_model_output = [0] * len(LABELS)\n",
        "\n",
        "# ...\n",
        "\n",
        "while True:\n",
        "    # ...\n",
        "\n",
        "    smoothed_softmax_model_output = exponential_smooth(\n",
        "        softmax_model_output, smoothed_softmax_model_output, ALPHA\n",
        "    )\n",
        "```\n",
        "\n",
        "A new exponentially smoothed classification class can be calculated using:\n",
        "\n",
        "```python\n",
        "smoothed_classification = smoothed_softmax_model_output.index(\n",
        "    max(smoothed_softmax_model_output)\n",
        ")\n",
        "```\n"
      ],
      "metadata": {
        "id": "yW2gExs4iaAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deciding when to ‚Äútype‚Äù an emoji\n",
        "\n",
        "We have an exponentially smooth classification value along with the model‚Äôs softmax output that accounts for high certainty model outputs. The exponential smoothed classification value can now be used to detect if a new emoji needs to be typed as follows: \n",
        "\n",
        "```python\n",
        "\n",
        "SMOOTHED_THRESHOLD = 0.80\n",
        "\n",
        "# ...\n",
        "\n",
        "last_output = -1\n",
        "\n",
        "# ...\n",
        "\n",
        "while True:\n",
        "    # ...\n",
        "\n",
        "    if (\n",
        "        smoothed_softmax_model_output[smoothed_classification] > SMOOTHED_THRESHOLD\n",
        "        and last_output is not smoothed_classification\n",
        "    ):\n",
        "        if smoothed_classification is not 0:\n",
        "            print(f\"Ready to send {LABELS[smoothed_classification]} emoji\")\n",
        "\n",
        "        last_output = smoothed_classification\n",
        "\n",
        "```\n",
        "\n",
        "This code first checks if the softmax output of smoothed classification value is above a desired threshold (0.80) and is also different from the previous output value - to prevent repeatedly sending the same emoji. If this criteria is met, and the classification value is not 0 (no gesture) we can send the emoji associated with the classification. The last output value is then updated with the new smoothed classification value, to use in the next loop cycle.\n"
      ],
      "metadata": {
        "id": "77uxXi5jj5lQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ‚ÄúTyping‚Äù Emojis over USB HID\n",
        "\n",
        "The firmware running on the OpenMV Cam H7 board does not enable USB HID by default. We can create a boot.py file on the OpenMV camera‚Äôs file system to enable USB HID:\n",
        "\n",
        "```python\n",
        "import pyb\n",
        "\n",
        "pyb.usb_mode('VCP+MSC+HID', hid=pyb.hid_keyboard)\n",
        "```\n",
        "\n",
        "This code uses the [`pyb` module](https://docs.openmv.io/library/pyb.html#)‚Äôs [`pyb.usb_mode(...)`](https://docs.openmv.io/library/pyb.html#pyb.usb_mode) API to enable USB HID in keyboard mode, while still enabling the USB VCP (virtual comm port) interface for serial communications and USB MSD (mass storage device) interface to enable access to the OpenMV board‚Äôs filesystem from a PC. Code in the boot.py file runs before the OpenMV application code.\n",
        "\n",
        "A USB HID keyboard message can now be sent by creating an instance of the [`pyb.USB_HID()`](https://docs.openmv.io/library/pyb.USB_HID.html) class and using the [`USB_HID.send(...)`](https://docs.openmv.io/library/pyb.USB_HID.html#pyb.USB_HID.send) API to send an 8 byte array value.\n",
        "\n",
        "At the time of writing this blog, there was no standard way to type an emoji character on all major operating systems (macOS, Linux, Windows). The next section will go over Operating System (OS) specific items. More information can be found on the [‚ÄúUnicode Input‚Äù Wikipedia page](https://en.wikipedia.org/wiki/Unicode_input).\n"
      ],
      "metadata": {
        "id": "xQEfNw2Kj-lU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### macOS\n",
        "\n",
        "It is possible to type emojis on macOS computers by enabling ‚Äú*Unicode Hex Input*‚Äù in System Preferences. Go into the ‚Äú*System Preferences*‚Äù application, click on the ‚Äú*Keyboard*‚Äù button and then the ‚ÄúInput Sources‚Äù tab.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/20.macos-keyboard-input-source-system-preferences.png?raw=1\" alt=\"macOS - Keyboard Input Source - System Preferences\" style=\"display: block; margin-left: auto; margin-right: auto; width: 300px;\">\n",
        "<figcaption style=\"text-align: center\"><i>macOS - Keyboard Input Source - System Preferences</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "Click the + button, then scroll down to the ‚Äú*Others*‚Äù category on the left hand pane, select ‚Äú*Unicode Hex Input*‚Äù, and click the ‚Äú*Add*‚Äù button.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/21.macos-system-preferences-add-unicode-hex-input-input-source.png?raw=1\" alt=\"macOS - System Preferences - Add Unicode Hex Input input source\" style=\"display: block; margin-left: auto; margin-right: auto; width: 300px;\">\n",
        "<figcaption style=\"text-align: center\"><i>macOS - System Preferences - Add ‚ÄúUnicode Hex Input‚Äù input source</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "A new item will appear on your Mac‚Äôs top menu bar for Keyboard inputs, select the newly added ‚Äú*Unicode Hex Input*‚Äù option.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/22.selecting-unicode-hex-input-in-the-macos-menu-bar.png?raw=1\" alt=\"Selecting Unicode Hex Input in the macOS menu bar\" style=\"display: block; margin-left: auto; margin-right: auto; width: 200px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Selecting ‚ÄúUnicode Hex Input‚Äù in the macOS menu bar</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "You will now be able to manually type emojis if you know their UTF-16 values. For example, the üëç emoji has a UTF-16 value of `0xd83ddc4d`. If you type hold down the option key and type this sequence: d,8,3,d,d,c,4,d - emoji will appear.\n"
      ],
      "metadata": {
        "id": "neh3-l7MkZrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linux\n",
        "\n",
        "On Linux it is possible to type emojis in applications that support UTF-8 input text (like LibreOffice) if you know their UTF-8 values. The üëç emoji has a UTF-8 value of `0x1f44d`. If you hold down the CTRL and SHIFT keys while typing u, and then type this sequence: 1,f,4,4,d - followed by a space character the emoji will appear."
      ],
      "metadata": {
        "id": "9bYGOzNFlDyE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Windows\n",
        "\n",
        "In applications like Word it is possible to type emojis by holding down the ALT key, pressing + key on the number pad, typing the UTF-8 value in decimal on the number pad key and releasing the ALT key. The üëç emoji has a UTF-8 value of `128077` (`0x1f44d` in hexadecimal). If you type this sequence while holding down the ALT key using the keys on the number pad: +,1,2,8,0,7,7 - the emoji will appear."
      ],
      "metadata": {
        "id": "GR13XZ1HlIiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Integrate the UnicodeHexKeyboard class\n",
        "Manually typing emojis via UTF-8 or UTF-16 codes is not convenient, however this can be automated using Python on the OpenMV Cam H7 board. We‚Äôve created [a custom `UnicodeHexKeyboard` Python class](https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/openmv/unicode_hex_keyboard.py) that handles everything for you.\n",
        "\n",
        "To use it, download the [`unicode_hex_key_board.py` file from the `openmv` folder on GitHub](https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/openmv/unicode_hex_keyboard.py) to the disk drive for the OpenMV board. The main application file can then use it as follows:\n",
        "\n",
        "```python\n",
        "import unicode_hex_keyboard\n",
        "\n",
        "# ...\n",
        "\n",
        "# keyboard instance to use to type emojis\n",
        "#  - to use with a Linux PC pass in: unicode_hex_keyboard.LINUX\n",
        "#  - to use with a Mac pass in: unicode_hex_keyboard.MACOS\n",
        "#  - to use with a Windows PC pass in: unicode_hex_keyboard.WINDOWS\n",
        "keyboard = unicode_hex_keyboard.UnicodeHexKeyboard(unicode_hex_keyboard.MACOS)\n",
        "\n",
        "# ...\n",
        "\n",
        "keyboard.send('üëç')\n",
        "```\n",
        "\n",
        "The final step is to update the code in the application‚Äôs main loop to send the classification specific emoji:\n",
        "\n",
        "```python\n",
        "# ...\n",
        "\n",
        "print(f'Ready to send {LABELS[smoothed_classification]} emoji')\n",
        "\n",
        "keyboard.send(LABELS[smoothed_classification])\n",
        "\n",
        "# ...\n",
        "```\n"
      ],
      "metadata": {
        "id": "Zk1_o1yUlTqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recap of Application\n",
        "\n",
        "We‚Äôve successfully integrated the TensorFlow Lite model into the OpenMV application. The application:\n",
        "\n",
        "1. Grabs an image frame from the camera\n",
        "2. Gets the ML model‚Äôs output for the captured image frame\n",
        "3. Filters the ML model‚Äôs output for high certainty predictions\n",
        "4. Uses an exponential smoothing function to smooth the model‚Äôs (softmax) outputs\n",
        "5. Uses the exponentially smoothed model outputs to determine if a new hand gesture is present.\n",
        "6. Then sends then ‚Äútypes‚Äù the associated emoji on a PC using the USB HID protocol.\n",
        "\n",
        "<figure>\n",
        "<img src=\"https://github.com/ArmDeveloperEcosystem/ml-image-classification-example-for-openmv/blob/main/images/23.block-diagram-of-application-processing-pipeline.png?raw=1\" alt=\"Block Diagram of Application processing pipeline\" style=\"display: block; margin-left: auto; margin-right: auto; width: 750px;\">\n",
        "<figcaption style=\"text-align: center\"><i>Block Diagram of Application processing pipeline</i></figcaption>\n",
        "</figure>\n",
        "\n",
        "You can save the `.py` file that you were editing and running manually on the board's USB disk interface as `main.py` now. When the board powers on, it will automatically start running the code in the `main.py` file. \n"
      ],
      "metadata": {
        "id": "QOBfY5yyl6hG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "Throughout this project we‚Äôve covered an end-to-end flow of training a custom image classification model and how to deploy it locally to a Arm Cortex-M7 based OpenMV development board using TensorFlow Lite! TensorFlow was used in a Google Colab notebook to train the model on a re-labeled public dataset from Kaggle. After training, the model was converted into TensorFlow Lite format to run on the OpenMV board using the TensorFlow Lite for Microcontrollers run-time along with accelerated Arm CMSIS-NN kernels. \n",
        " \n",
        "At inference time the model‚Äôs outputs were processed using model certainty techniques, and then fed output from the (Softmax) activation output into an exponential smoothing function to determine when to send keystrokes over USB HID to type emojis on a PC. The dedicated input device we created was able to capture and process grayscale 96x96 image data at just under 20 fps on an Arm Cortex-M7 processor running at 480 MHz. On-device inferencing provided a low latency response and preserved the privacy of the user by keeping all image data at the source and processing it locally. \n",
        "\n",
        "Build one yourself by purchasing an OpenMV Cam H7 R2 board on [openmv.io](https://openmv.io/collections/products/products/openmv-cam-h7-r2) or [a distributor](https://openmv.io/collections/products). The project can be extended by fine tuning the model on your own data or applying transfer learning techniques and using the model we developed as base to train other hand gestures. Maybe you can find another public dataset for facial gestures and use it to type üòÄ emojis when you smile! \n",
        "\n",
        "*A big thanks to Sparsh Gupta for sharing the Gesture Recognition dataset on Kaggle under a public domain license and my Arm colleagues Rod Crawford, Prathyusha Venkata, Elham Harirpoush, and Liliya Wu for their help in reviewing the material for this guide!*\n",
        "\n"
      ],
      "metadata": {
        "id": "LufVZttWoOwW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Reading\n",
        "\n",
        "* Papers\n",
        "  * [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)\n",
        "  * [Visual Wake Words Dataset](https://arxiv.org/abs/1906.05721)\n",
        "* Books\n",
        "  * [Human-in-the-Loop Machine Learning by Robert (Munro) Monarch](https://www.manning.com/books/human-in-the-loop-machine-learning)"
      ],
      "metadata": {
        "id": "RzZggQZzoXd3"
      }
    }
  ]
}